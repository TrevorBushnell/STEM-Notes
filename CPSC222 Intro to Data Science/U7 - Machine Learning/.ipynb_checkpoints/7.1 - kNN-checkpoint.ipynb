{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.1: kNN ML Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to ML (Machine Learning)\n",
    "\n",
    "* Supervised learning: labeled data (e.g. there is an attirbute (AKA feature) that you are interested in predicting for unseen instances)\n",
    "    * The attribute is called the \"class\" or the \"class label\"\n",
    "    * the attribute is categorical... classification\n",
    "    * The attirbute is numeric... regression\n",
    "    * Example algorithm: kNN (k nearest neighbors)\n",
    "\n",
    "* Unsupervised learning: unlabeled data\n",
    "    * Example algorithm: k-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning\n",
    "\n",
    "* Need a way to divide a dataset into a training set and a testing set\n",
    "    * The training set is used to build/train an/a algorithm/model\n",
    "    * The testing set is used to evaluate the algorithm/model\n",
    "    * The training set and the testing set *are different*\n",
    "* Example: \n",
    "    * We have a super-tiny t-shirt sizes dataset\n",
    "        * 4 instances\n",
    "        * 3 attributes (1 is the class (t-shirt size))\n",
    "        * Goal is to use height and weight attributes to predict t-shirt size\n",
    "        * We will do this for a test set with a single unseen instance\n",
    "            * height=161 weight=63 t-shirt size=?\n",
    "            * Let's say the \"ground truth value\" is M (medium)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN Algorithm\n",
    "\n",
    "* Identify the $k$ nearest neighbors in the training set to a set set instance\n",
    "    * The most frequently occuring class label amonst the $k$ nearest neighbors will be the clas slabel prediction for the unseen instance\n",
    "* We need a way to measure \"nearness AKA \"closeness\"\n",
    "    * 2D: $\\sqrt{a^2 + b^2}$\n",
    "    * ND: Euclidean distance - $\\sqrt{\\sum_{i=1}^n (a_i-b_i)^2}$\n",
    "* We need to normalize (AKA scale) our attirbutes so we don't have unanticipated weighting of one attribute more than another (e.g. height has a larger scale than weight so it will dominate the formula)\n",
    "    * We will use the min-max scaling approach\n",
    "    * For each attribute, the min becomes 0 and the max becomes 1 (e.g. bounded to $[0,1]$ so the units have no weighting effect)\n",
    "    * For each attribute, for each value, subtract the min then divide by the original range (max - min)\n",
    "\n",
    "\n",
    "\n",
    "### Tracing the kNN Algorithm\n",
    "\n",
    "| row # | height (m) | wight (kg) | t-shirt size |\n",
    "| - | - | - | - |\n",
    "| 0 | 158 | 58 | M |\n",
    "| 1 | 163 | 61 | M | \n",
    "| 2 | 165 | 61 | L |\n",
    "| 3 | 168 | 66 | L |\n",
    "\n",
    "**STEP 1: Scale the height**\n",
    "\n",
    "* For HEIGHT:\n",
    "    * min = 158\n",
    "    * max = 168\n",
    "    * range = 10\n",
    "\n",
    "* For WEIGHT:\n",
    "    * min = 58\n",
    "    * max = 66 \n",
    "    * k = 8\n",
    "\n",
    "| row # | height (m) | wight (kg) | t-shirt size |\n",
    "| - | - | - | - |\n",
    "| 0 | $\\frac{158-158}{10} = 0$ | $\\frac{58-58}{8} = 0$ | M |\n",
    "| 1 | $\\frac{163-158}{10} = 0.5$ | $\\frac{61-58}{8} = 0.375$ | M | \n",
    "| 2 | $\\frac{165-158}{10} = 0.7$ | $\\frac{61-58}{8} = 0.375$ | L |\n",
    "| 3 | $\\frac{168-158}{10} = 1$ | $\\frac{66-58}{8} = 1$ | L |\n",
    "\n",
    "\n",
    "Our UNSEEN INSTANCE: $(161, 63) = (\\frac{161-158}{10}, \\frac{58-63}{8}) = (0.3, 0.625)$\n",
    "\n",
    "**STEP 2: Calculate the distance**\n",
    "\n",
    "| row # | height (m) | wight (kg) | t-shirt size | Distance from $(0.3, 0.625)$|\n",
    "| - | - | - | - | - |\n",
    "| 0 | 0 | 0 | M | $\\sqrt{(0-0.3)^2 + (0-0.625)^2} = 0.6933$ |\n",
    "| 1 | 0.5 | 0.375 | M | $\\sqrt{(0.5-0.3)^2 + (0.375-0.625)^2} = 0.32$ | \n",
    "| 2 | 0.7 | 0.375 | L | $\\sqrt{(0.7-0.3)^2 + (0.375-0.625)^2} = 0.47$ |\n",
    "| 3 | 1 | 1 | L | $\\sqrt{(1-0.3)^2 + (1-0.625)^2} = 0.79$ |\n",
    "\n",
    "**STEP 3: Get the class labels for the 3 smallest distances**\n",
    "\n",
    "* If we look at the table directly above, then the closest rows are 1, 2, and 0. The class labels are M, M, and L.\n",
    "\n",
    "\n",
    "**STEP 4: Pick the majority class and use that as the prediction**\n",
    "\n",
    "* Since the majority are M, our prediction will be M\n",
    "\n",
    "\n",
    "## Coding the kNN Algorithm\n",
    "\n",
    "We will start by getting all of our data appropriately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset:\n",
      "    height(cm)  weight(kg)\n",
      "0         158          58\n",
      "1         163          61\n",
      "2         165          61\n",
      "3         168          66\n",
      "\n",
      "training y-vector:\n",
      " 0    M\n",
      "1    M\n",
      "2    L\n",
      "3    L\n",
      "Name: t-shirt size, dtype: object\n",
      "\n",
      "test data:\n",
      " [[161, 63]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# knn algorithm with the scikit-learn library\n",
    "# notation\n",
    "# X: a feature matrix (rows of feature vectors (instances)) with the class labels stripped off\n",
    "# y: is a class label vector\n",
    "# X and y are parallel\n",
    "# use _train and _test to denote train and test sets despectively\n",
    "\n",
    "df = pd.read_csv('shirt_sizes.csv')\n",
    "\n",
    "X_train = df.drop(\"t-shirt size\", axis=1) # 1 is for columns\n",
    "print('training dataset:\\n', X_train)\n",
    "\n",
    "y_train = df[\"t-shirt size\"]\n",
    "print('\\ntraining y-vector:\\n', y_train)\n",
    "\n",
    "X_test = [[161, 63]]\n",
    "print('\\ntest data:\\n', X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will normalize our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized training data:\n",
      " [[0.    0.   ]\n",
      " [0.5   0.375]\n",
      " [0.7   0.375]\n",
      " [1.    1.   ]]\n",
      "\n",
      "Normalized test data:\n",
      " [[0.3   0.625]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_normalized = scaler.transform(X_train)\n",
    "print('Normalized training data:\\n', X_train_normalized)\n",
    "X_test_normalized = scaler.transform(X_test)\n",
    "print('\\nNormalized test data:\\n', X_test_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then set up the kNN classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(metric='euclidean', n_neighbors=3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=3, metric='euclidean')\n",
    "knn_clf.fit(X_train_normalized, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to make our prediction with the classifier for the unseen instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y predicted: ['M']\n",
      "nearest neighbors: (array([[0.32015621, 0.47169906, 0.69327123]]), array([[1, 2, 0]]))\n"
     ]
    }
   ],
   "source": [
    "y_predicted = knn_clf.predict(X_test_normalized)\n",
    "print('y predicted:', y_predicted)\n",
    "print('nearest neighbors:', knn_clf.kneighbors(X_test_normalized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing Throughts on kNN\n",
    "\n",
    "* What if our attributes are not numeric (meaning they are categorical)?\n",
    "    * simple approach: map labels to integers\n",
    "        * `from sklearn.preprocessing import LabelEncoder`\n",
    "    * Another approach: write your own distance function(0 if labels are same, 1 otherwise)\n",
    "* kNN is NOT the only ML algorithm\n",
    "    * Naive Bayes\n",
    "    * Descision trees (random forests)\n",
    "    * SVMs (support vector machines)\n",
    "    * Neural networks (deep learning)\n",
    "    * etc."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
