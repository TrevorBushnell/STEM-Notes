{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.1: Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = [\"height(cm)\", \"weight(kg)\", \"size(t-shirt)\"]\n",
    "table = [\n",
    " [158, 58], # \"M\"\n",
    " [158, 59], # \"M\"\n",
    " [158, 63], # \"M\"\n",
    " [160, 59], # \"M\"\n",
    " [160, 60], # \"M\"\n",
    " [163, 60], # \"M\"\n",
    " [163, 61], # \"M\"\n",
    " [160, 64], # \"L\"\n",
    " [163, 64], # \"L\"\n",
    " [165, 61], # \"L\"\n",
    " [165, 62], # \"L\"\n",
    " [165, 65], # \"L\"\n",
    " [168, 62], # \"L\"\n",
    " [168, 63], # \"L\"\n",
    " [168, 66], # \"L\"\n",
    " [170, 63], # \"L\"\n",
    " [170, 64], # \"L\"\n",
    " [170, 68] # \"L\"\n",
    "]\n",
    "\n",
    "import random\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Clustering\n",
    "\n",
    "**WHY?** - EDA, data reduction, similarity search, prediction\n",
    "\n",
    "Keep in mind that this is very much different from kNN classification because there could be different sizes of clusters\n",
    "\n",
    "> **THE K-MEANS CLUSTERING ALGORITHM**\n",
    ">\n",
    "> 1. Pick a $k$ (e.g. $k=2$)\n",
    "> 2. Select $k$ (random) isntances for the initial cluster centroids (has an effect on the final resulting clusters... not guaranteed to find the \"global\" optimum solution, just a \"local\" optimal solution)\n",
    "> 3. Assign each instance to the cluster with the \"nearest\" centroid\n",
    "> 4. Re-calculate the centroids\n",
    "> 5. Repeat steps #3-4 until the centroids no longer \"move\"\n",
    "\n",
    "* **centroid:** the center point of a cluster, aka the average point in the cluster\n",
    "\n",
    "> **DETERMINING THE QUALITY OF A CLUSTER**\n",
    "> The quality of the cluster is given by the TSS (total sum of squares) function:\n",
    ">\n",
    "> $$ TSS = \\sum_{i=1}^n ((x_i - \\overline x)^2 + (y_1 - \\overline y)^2) $$\n",
    "\n",
    "* We don't want $k=1$ or $k=n$ because everything in 1 cluster and one instance per cluster doesn't make sense\n",
    "* However, $k \\in [2, n-1]$ would have valuable clusters, but we don't know until we try it\n",
    "* As such, run the k-means algorithm and measure objective function (TSS over all clusters)\n",
    "    * make a plot of $k$ on x and $TSS$ on y\n",
    "* The most optimal point is called the *elbow point*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Represent Clusters in Python\n",
    "1. A list of instances, 1 list per cluster\n",
    "2. add a column to the table for the cluster number\n",
    "    * I would really like to do this method I think\n",
    "    * This is close to how sklearn does this algorithm\n",
    "    * Then we can use a group by :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_k_means_clustering(table, k):\n",
    "    table = [row + [None] for row in table]\n",
    "\n",
    "    # STEP 2: Select k random instances\n",
    "    random_instances = random.sample(table, 2)\n",
    "\n",
    "    for i in range(len(random_instances)):\n",
    "        random_instances[i][-1] = i\n",
    "    for row in table:\n",
    "        print(row)\n",
    "\n",
    "    # STEP 3\n",
    "    cluster_centers_ = compute_cluster_centroids(table, k) # all doing is grouping by cluster # and averaging the instances per group\n",
    "    for instance in table:\n",
    "        nearest_cluster_num = find_nearest_cluster(instance, cluster_centers) # use distance and sorting logic from knn here\n",
    "        # update instance's cluster num\n",
    "\n",
    "    # STEP 4\n",
    "    new_cluster_centers = compute_cluster_centroids(table, k)\n",
    "\n",
    "    # STEP 5\n",
    "    moved = check_clusters_moved(cluster_centers, new_cluster_centers)\n",
    "    while moved:\n",
    "        # Repeat steps 3-4CPSC\n",
    "\n",
    "    return [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[158, 58, None]\n",
      "[158, 59, None]\n",
      "[158, 63, None]\n",
      "[160, 59, None]\n",
      "[160, 60, None]\n",
      "[163, 60, None]\n",
      "[163, 61, None]\n",
      "[160, 64, None]\n",
      "[163, 64, None]\n",
      "[165, 61, None]\n",
      "[165, 62, None]\n",
      "[165, 65, None]\n",
      "[168, 62, 0]\n",
      "[168, 63, 1]\n",
      "[168, 66, None]\n",
      "[170, 63, None]\n",
      "[170, 64, None]\n",
      "[170, 68, None]\n",
      "Cluster numbers: []\n",
      "centroids: []\n"
     ]
    }
   ],
   "source": [
    "k = 2\n",
    "labels_, cluster_centers_ = perform_k_means_clustering(table, k)\n",
    "print('Cluster numbers:', labels_)\n",
    "print('centroids:', cluster_centers_)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a707b6ce8c685eb936424fcc3009d4b4b7a52543c4db09380a3fc49186ceb509"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
