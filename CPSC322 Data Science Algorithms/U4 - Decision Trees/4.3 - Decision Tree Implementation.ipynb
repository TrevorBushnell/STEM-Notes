{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3: Decision Tree Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Statements and Starter Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = [\"level\", \"lang\", \"tweets\", \"phd\"]\n",
    "attribute_domains = {\"level\": [\"Senior\", \"Mid\", \"Junior\"], \n",
    "        \"lang\": [\"R\", \"Python\", \"Java\"],\n",
    "        \"tweets\": [\"yes\", \"no\"], \n",
    "        \"phd\": [\"yes\", \"no\"]}\n",
    "X_train = [\n",
    "        [\"Senior\", \"Java\", \"no\", \"no\"],\n",
    "        [\"Senior\", \"Java\", \"no\", \"yes\"],\n",
    "        [\"Mid\", \"Python\", \"no\", \"no\"],\n",
    "        [\"Junior\", \"Python\", \"no\", \"no\"],\n",
    "        [\"Junior\", \"R\", \"yes\", \"no\"],\n",
    "        [\"Junior\", \"R\", \"yes\", \"yes\"],\n",
    "        [\"Mid\", \"R\", \"yes\", \"yes\"],\n",
    "        [\"Senior\", \"Python\", \"no\", \"no\"],\n",
    "        [\"Senior\", \"R\", \"yes\", \"no\"],\n",
    "        [\"Junior\", \"Python\", \"yes\", \"no\"],\n",
    "        [\"Senior\", \"Python\", \"yes\", \"yes\"],\n",
    "        [\"Mid\", \"Python\", \"no\", \"yes\"],\n",
    "        [\"Mid\", \"Java\", \"yes\", \"no\"],\n",
    "        [\"Junior\", \"Python\", \"no\", \"yes\"]\n",
    "    ]\n",
    "\n",
    "y_train = [\"False\", \"False\", \"True\", \"True\", \"True\", \"False\", \"True\", \"False\", \"True\", \"True\", \"True\", \"True\", \"True\", \"False\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do We Represent Trees in Python?\n",
    "1. nested data structure (e.g. lists)\n",
    "2. OOP (e.g. `MyTree` class)\n",
    "\n",
    "for our intents and purposes, we will use a nested tree implementation. \n",
    "* At elem 0, we have the \"data type\" (Attribute, Value, Leaf)\n",
    "* At elem 1, we have the \"data\" (attribute name, attribute value, class label)\n",
    "* At elem 2, it depends on the data type\n",
    "\n",
    "##### Example: Tree Solution for the Interview Dataset\n",
    "\n",
    "This is based on the decision tree that we made in Lab Task #1 of 4.2!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "interview_tree_solution = [\"Attribute\", \"level\", \n",
    "                            [\"Value\", \"Senior\", \n",
    "                                [\"Attribute\", \"tweets\", \n",
    "                                    [\"Value\", \"yes\", \n",
    "                                        [\"Leaf\", \"True\", 2, 5]\n",
    "                                    ],\n",
    "                                    [\"Value\", \"no\", \n",
    "                                        [\"Leaf\", \"False\", 3, 5]\n",
    "                                    ]\n",
    "                                ]\n",
    "                            ],\n",
    "                            [\"Value\", \"Mid\", \n",
    "                                [\"Leaf\", \"True\", 4, 14]\n",
    "                            ],\n",
    "                            [\"Value\", \"Junior\", \n",
    "                                [\"Attribute\", \"phd\", \n",
    "                                    [\"Value\", \"yes\", \n",
    "                                        [\"Leaf\", \"False\", 2, 5]\n",
    "                                    ],\n",
    "                                    [\"Value\", \"no\", \n",
    "                                        [\"Leaf\", \"True\", 3, 5]\n",
    "                                    ]\n",
    "                                ]\n",
    "                            ]\n",
    "                        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTIONS\n",
    "\n",
    "def select_attribute(instances, attributes):\n",
    "    # TODO: USE ENTROPY TO COMPUTE/CHOOSE ATTRIBUTE WITH THE SMALLEST E_new\n",
    "    # for now... just choose randomly\n",
    "    return np.random.choice(attributes) # my code\n",
    "\n",
    "    # OR...\n",
    "    # rand_index = np.random.randint(0, len(attributes))\n",
    "    # return attributes[rand_index]\n",
    "\n",
    "def partition_instances(instances, split_attribute):\n",
    "    # let's use a dictionary\n",
    "    partitions = {} # key (string) : value (subtable)\n",
    "    att_index = header.index(split_attribute) # e.g. 0 for level\n",
    "    att_domain = attribute_domains[att_index] # e.g. ['Junior', 'Mid', 'Senior']\n",
    "    for att_value in att_domain:\n",
    "        partitions[att_value] = []\n",
    "        for instance in instances:\n",
    "            if instance[att_index] == att_value:\n",
    "                partitions[att_value].append(instance)\n",
    "    return partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_domains = {\n",
    "                        0: [\"Junior\", \"Mid\", \"Senior\"],\n",
    "                        1: [\"Java\", \"Python\", \"R\"],\n",
    "                        2: [\"no\", \"yes\"],\n",
    "                        3: [\"no\", \"yes\"]\n",
    "                    }\n",
    "                    # NOTE: can also use strings here instead of integers\n",
    "\n",
    "def tdidt(current_instances, available_attributes):\n",
    "    # basic approach (uses recursion!!):\n",
    "    print(\"available attributes:\", available_attributes)\n",
    "    \n",
    "    # select an attribute to split on\n",
    "    attribute = select_attribute(current_instances, available_attributes)\n",
    "    print(\"splitting on attribute\", attribute)\n",
    "    available_attributes.remove(attribute)\n",
    "    tree = [\"Attribute\", attribute]\n",
    "    # group data by attribute domains (creates pairwise disjoint partitions)\n",
    "    partitions = partition_instances(current_instances, attribute)\n",
    "    print('partitions:', partitions)\n",
    "    # for each partition, repeat unless one of the following occurs (base case)\n",
    "    for att_value, att_partition in partitions.items():\n",
    "        print('current attribute value:', att_value, len(att_partition))\n",
    "        value_subtree = [\"Value\", att_value]\n",
    "    #    CASE 1: all class labels of the partition are the same => make a leaf node\n",
    "    if len(att_partition) > 0 and all_same_class(att_partition):\n",
    "        pass\n",
    "    #    CASE 2: no more attributes to select (clash) => handle clash w/majority vote leaf node\n",
    "    elif len(att_partition) > 0 and len(available_attributes) == 0:\n",
    "        print(\"CASE 2 no more attributes\")\n",
    "        # TODO: ???? \n",
    "    #    CASE 3: no more instances to partition (empty partition) => backtrack and replace attribute node with majority vote leaf node\n",
    "    elif len(att_partition) == 0:\n",
    "        print('CASE 3: empty partition')\n",
    "        # TODO: backtrack to replace the attribute code\n",
    "        # with a majority vote leaf node\n",
    "        # tree = [\"Attribute\", attribute] <- replace this with a majority vote leaf node\n",
    "    else: # Previous conditions all false so THIS IS THE RECURSIVE STEP\n",
    "        subtree = tdidt()\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Notes on `fit()`\n",
    "\n",
    "* Only takes `X_train` and `y_train`, does NOT take any headers\n",
    "    * ... as such we need to extract the headers (e.g. `[\"att0\", \"att1\", ...]`)\n",
    "* Additionally you need to extract the domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available attributes: ['level', 'lang', 'tweets', 'phd']\n",
      "splitting on attribute level\n",
      "tree None\n"
     ]
    }
   ],
   "source": [
    "def fit_starter_code():\n",
    "    # would advise that X_train and y_train get stitched together\n",
    "    train = [X_train[i] + [y_train[i]] for i in range(len(X_train))]\n",
    "    # next make a copy of your header since tdidt will modify the list\n",
    "    available_attributes = header.copy()\n",
    "    tree = tdidt(train, available_attributes)\n",
    "    print(\"tree\", tree)\n",
    "\n",
    "fit_starter_code()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General $E_{new}$ Algorithm\n",
    "\n",
    "* For each available attribute:\n",
    "    * For each attribute value in the domain:\n",
    "        * Compute the entropy of that value partition (e.g. proportion and log for each class)\n",
    "    * compute the $E_{new}$ by taking weighted sum of the partition entropies\n",
    "* Choose to split on the attribute with the smallest $E_{new}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Tips on PA7\n",
    "\n",
    "1. `all_same_class()`\n",
    "2. append subtree to `value_subtree` and to tree appropriately\n",
    "3. work on CASE 1, then CASE 2, then CASE 3 (write helper functions!)\n",
    "4. finish the TODOs in `fit_starter_code()`\n",
    "5. replace random w/ entropy (compare tree w/ `interview_tree_solution`)\n",
    "6. Implement unit test for `fit()` and move start code over to OOP\n",
    "7. move on to `predict()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Final Tree Topics\n",
    "\n",
    "### Tree Visualization\n",
    "\n",
    "* we will specify a tree using the dot language\n",
    "* then we will create a pdf using the dot program\n",
    "* will be BONUS for PA7 (but still highly recommended)\n",
    "    * if it wasn't bonus, it would be inserted inbetween 3 and 4\n",
    "* EXAMPLE: the following are inside the contents of `interview_tree.dot`:\n",
    "\n",
    "```\n",
    "graph g {\n",
    "    level [shape=box];\n",
    "    phd[shape=box];\n",
    "    level -- phd [label=\"Junior\"]\n",
    "    \n",
    "    true1 [label=\"True\"]\n",
    "    false1 [label=\"False\"]\n",
    "    phd -- true1 [label=\"no\"]\n",
    "    phd -- false1 [label=\"yes\"]\n",
    "\n",
    "    true2 [label=\"True\"]\n",
    "    level -- true2 [label=\"Mid\"]\n",
    "\n",
    "    tweets [shape=box]\n",
    "    level -- tweets [label=\"Senior\"]\n",
    "    true3 [label=\"True\"]\n",
    "    false3 [label=\"False\"]\n",
    "    tweets -- false3 [label=\"no\"]\n",
    "    tweets -- true3 [label=\"yes\"]\n",
    "}\n",
    "```\n",
    "\n",
    "Then, run the following command in your docker container:\n",
    "```\n",
    "dot -Tpdf interview_tree.dot -o interview_tree.pdf\n",
    "```\n",
    "\n",
    "\n",
    "### Pruning\n",
    "\n",
    "* Decision trees are notorious for overfitting to a training set\n",
    "    * $\\therefore$ trees don't generalize well to unseen instances\n",
    "* we can combat this issue by doing **post-pruning** with a *pruining set*\n",
    "* There is no pruning programming part on PA7"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a707b6ce8c685eb936424fcc3009d4b4b7a52543c4db09380a3fc49186ceb509"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
