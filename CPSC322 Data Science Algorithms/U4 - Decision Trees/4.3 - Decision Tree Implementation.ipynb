{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3: Decision Tree Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Statements and Starter Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = [\"level\", \"lang\", \"tweets\", \"phd\"]\n",
    "attribute_domains = {\"level\": [\"Senior\", \"Mid\", \"Junior\"], \n",
    "        \"lang\": [\"R\", \"Python\", \"Java\"],\n",
    "        \"tweets\": [\"yes\", \"no\"], \n",
    "        \"phd\": [\"yes\", \"no\"]}\n",
    "X_train = [\n",
    "        [\"Senior\", \"Java\", \"no\", \"no\"],\n",
    "        [\"Senior\", \"Java\", \"no\", \"yes\"],\n",
    "        [\"Mid\", \"Python\", \"no\", \"no\"],\n",
    "        [\"Junior\", \"Python\", \"no\", \"no\"],\n",
    "        [\"Junior\", \"R\", \"yes\", \"no\"],\n",
    "        [\"Junior\", \"R\", \"yes\", \"yes\"],\n",
    "        [\"Mid\", \"R\", \"yes\", \"yes\"],\n",
    "        [\"Senior\", \"Python\", \"no\", \"no\"],\n",
    "        [\"Senior\", \"R\", \"yes\", \"no\"],\n",
    "        [\"Junior\", \"Python\", \"yes\", \"no\"],\n",
    "        [\"Senior\", \"Python\", \"yes\", \"yes\"],\n",
    "        [\"Mid\", \"Python\", \"no\", \"yes\"],\n",
    "        [\"Mid\", \"Java\", \"yes\", \"no\"],\n",
    "        [\"Junior\", \"Python\", \"no\", \"yes\"]\n",
    "    ]\n",
    "\n",
    "y_train = [\"False\", \"False\", \"True\", \"True\", \"True\", \"False\", \"True\", \"False\", \"True\", \"True\", \"True\", \"True\", \"True\", \"False\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do We Represent Trees in Python?\n",
    "1. nested data structure (e.g. lists)\n",
    "2. OOP (e.g. `MyTree` class)\n",
    "\n",
    "for our intents and purposes, we will use a nested tree implementation. \n",
    "* At elem 0, we have the \"data type\" (Attribute, Value, Leaf)\n",
    "* At elem 1, we have the \"data\" (attribute name, attribute value, class label)\n",
    "* At elem 2, it depends on the data type\n",
    "\n",
    "##### Example: Tree Solution for the Interview Dataset\n",
    "\n",
    "This is based on the decision tree that we made in Lab Task #1 of 4.2!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "interview_tree_solution = [\"Attribute\", \"level\", \n",
    "                            [\"Value\", \"Senior\", \n",
    "                                [\"Attribute\", \"tweets\", \n",
    "                                    [\"Value\", \"yes\", \n",
    "                                        [\"Leaf\", \"True\", 2, 5]\n",
    "                                    ],\n",
    "                                    [\"Value\", \"no\", \n",
    "                                        [\"Leaf\", \"False\", 3, 5]\n",
    "                                    ]\n",
    "                                ]\n",
    "                            ],\n",
    "                            [\"Value\", \"Mid\", \n",
    "                                [\"Leaf\", \"True\", 4, 14]\n",
    "                            ],\n",
    "                            [\"Value\", \"Junior\", \n",
    "                                [\"Attribute\", \"phd\", \n",
    "                                    [\"Value\", \"yes\", \n",
    "                                        [\"Leaf\", \"False\", 2, 5]\n",
    "                                    ],\n",
    "                                    [\"Value\", \"no\", \n",
    "                                        [\"Leaf\", \"True\", 3, 5]\n",
    "                                    ]\n",
    "                                ]\n",
    "                            ]\n",
    "                        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTIONS\n",
    "\n",
    "def select_attribute(instances, attributes):\n",
    "    # TODO: USE ENTROPY TO COMPUTE/CHOOSE ATTRIBUTE WITH THE SMALLEST E_new\n",
    "    # for now... just choose randomly\n",
    "    return np.random.choice(attributes) # my code\n",
    "\n",
    "    # OR...\n",
    "    # rand_index = np.random.randint(0, len(attributes))\n",
    "    # return attributes[rand_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_domains = {\n",
    "                        0: [\"Junior\", \"Mid\", \"Senior\"],\n",
    "                        1: [\"Java\", \"Python\", \"R\"],\n",
    "                        2: [\"no\", \"yes\"],\n",
    "                        3: [\"no\", \"yes\"]\n",
    "                    }\n",
    "\n",
    "def tdidt(current_instances, available_attributes):\n",
    "    # basic approach (uses recursion!!):\n",
    "    print(\"available attributes:\", available_attributes)\n",
    "    \n",
    "    # select an attribute to split on\n",
    "    attribute = select_attribute(current_instances, available_attributes)\n",
    "    print(\"splitting on attribute\", attribute)\n",
    "    available_attributes.remove(attribute)\n",
    "    tree = [\"Attribute\", attribute]\n",
    "    # group data by attribute domains (creates pairwise disjoint partitions)\n",
    "    # for each partition, repeat unless one of the following occurs (base case)\n",
    "    #    CASE 1: all class labels of the partition are the same => make a leaf node\n",
    "    #    CASE 2: no more attributes to select (clash) => handle clash w/majority vote leaf node\n",
    "    #    CASE 3: no more instances to partition (empty partition) => backtrack and replace attribute node with majority vote leaf node\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Notes on `fit()`\n",
    "\n",
    "* Only takes `X_train` and `y_train`, does NOT take any headers\n",
    "    * ... as such we need to extract the headers (e.g. `[\"att0\", \"att1\", ...]`)\n",
    "* Additionally you need to extract the domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_starter_code():\n",
    "    # would advise that X_train and y_train get stitched together\n",
    "    train = [X_train[i] + [y_train[i]] for i in range(len(X_train))]\n",
    "    # next make a copy of your header since tdidt will modify the list\n",
    "    available_attributes = header.copy()\n",
    "    tree = tdidt(train, available_attributes)\n",
    "    print(\"tree\", tree)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a707b6ce8c685eb936424fcc3009d4b4b7a52543c4db09380a3fc49186ceb509"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
