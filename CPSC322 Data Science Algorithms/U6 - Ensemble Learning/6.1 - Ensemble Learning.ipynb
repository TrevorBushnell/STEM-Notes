{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1: Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# pasted from DecisionTreeFun\n",
    "header = [\"level\", \"lang\", \"tweets\", \"phd\"]\n",
    "attribute_domains = {\"level\": [\"Senior\", \"Mid\", \"Junior\"], \n",
    "    \"lang\": [\"R\", \"Python\", \"Java\"],\n",
    "    \"tweets\": [\"yes\", \"no\"], \n",
    "    \"phd\": [\"yes\", \"no\"]}\n",
    "X = [\n",
    "    [\"Senior\", \"Java\", \"no\", \"no\"],\n",
    "    [\"Senior\", \"Java\", \"no\", \"yes\"],\n",
    "    [\"Mid\", \"Python\", \"no\", \"no\"],\n",
    "    [\"Junior\", \"Python\", \"no\", \"no\"],\n",
    "    [\"Junior\", \"R\", \"yes\", \"no\"],\n",
    "    [\"Junior\", \"R\", \"yes\", \"yes\"],\n",
    "    [\"Mid\", \"R\", \"yes\", \"yes\"],\n",
    "    [\"Senior\", \"Python\", \"no\", \"no\"],\n",
    "    [\"Senior\", \"R\", \"yes\", \"no\"],\n",
    "    [\"Junior\", \"Python\", \"yes\", \"no\"],\n",
    "    [\"Senior\", \"Python\", \"yes\", \"yes\"],\n",
    "    [\"Mid\", \"Python\", \"no\", \"yes\"],\n",
    "    [\"Mid\", \"Java\", \"yes\", \"no\"],\n",
    "    [\"Junior\", \"Python\", \"no\", \"yes\"]\n",
    "]\n",
    "\n",
    "y = [\"False\", \"False\", \"True\", \"True\", \"True\", \"False\", \"True\", \"False\", \"True\", \"True\", \"True\", \"True\", \"True\", \"False\"]\n",
    "# stitch X and y together to make one table\n",
    "table = [X[i] + [y[i]] for i in range(len(X))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Ensemble Learning\n",
    "\n",
    "* Different classifiers have strengths/weaknesses\n",
    "* Instead of using one classifier, use multiple different classifiers\n",
    "* Use voting to select prediction results\n",
    "* Leads to better prediction results\n",
    "\n",
    "**EXAMPLES:**\n",
    "* Entropy tends to result in smaller decision trees\n",
    "* Leads to \"better\" predictions... but not guaranteed\n",
    "* Instead use differen attribute selection approaches, and choose the \"most popular prediction (among the ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approaches to Ensemble Learning\n",
    "\n",
    "* If the classifiers are of the same type (e.g. decision trees), the ensemble is **homogeneous** (our focus)\n",
    "\n",
    "* If different types, the ensemble is **heterogeneous**\n",
    "\n",
    "Techniques we will be looking at:\n",
    "* Bagging (Bootstrap Aggregation)\n",
    "* Random Forests\n",
    "\n",
    "Different voting schemes:\n",
    "* (simple) majority voting\n",
    "* weighted voting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging (Bootstrap Aggregation)\n",
    "\n",
    "Recall the bootstrap method:\n",
    "\n",
    "* BLAH BLAH BLAH\n",
    "\n",
    "> **Bagging Algorithm**\n",
    ">\n",
    "> * Generate $k$ classifiers\n",
    "> * Each classifier $M_i$ is trained on $D_i$ ($for 1 \\le i \\le k$)\n",
    "> * $D_i$ is a bootstrap sample\n",
    ">\n",
    ">\n",
    "> In order to classify an instance $X$:\n",
    "> * Run each classifier $M_i$ on $X$ to get predicted label $L_i$\n",
    "> * Each label $L_i$ is a vote for that label\n",
    "> * Use the majority label (i.e., the mode) as the prediction\n",
    "\n",
    "### Advantages of Bagging\n",
    "* simple idea, simple to implement\n",
    "* reduces overfitting\n",
    "* generally increases accuracy\n",
    "* reduces classification variance across classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Lab Task #1:** Write a function that returns a random sample of rows with replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mid', 'R', 'yes', 'yes', 'True']\n",
      "['Senior', 'Java', 'no', 'no', 'False']\n",
      "['Junior', 'R', 'yes', 'yes', 'False']\n",
      "['Junior', 'Python', 'no', 'no', 'True']\n",
      "['Junior', 'Python', 'no', 'yes', 'False']\n",
      "['Junior', 'Python', 'no', 'no', 'True']\n",
      "['Junior', 'Python', 'no', 'no', 'True']\n",
      "['Senior', 'R', 'yes', 'no', 'True']\n",
      "['Senior', 'Python', 'no', 'no', 'False']\n",
      "['Senior', 'Java', 'no', 'yes', 'False']\n",
      "['Senior', 'Python', 'yes', 'yes', 'True']\n",
      "['Mid', 'Python', 'no', 'no', 'True']\n",
      "['Junior', 'R', 'yes', 'yes', 'False']\n",
      "['Junior', 'Python', 'yes', 'no', 'True']\n"
     ]
    }
   ],
   "source": [
    "def compute_bootstrapped_sample(table):\n",
    "    n = len(table)\n",
    "    sample = []\n",
    "    for _ in range(n):\n",
    "        rand_index = np.random.randint(0, n) # Return random integers from low (inclusive) to high (exclusive)\n",
    "        sample.append(table[rand_index])\n",
    "    return sample \n",
    "\n",
    "sample = compute_bootstrapped_sample(table)\n",
    "for row in sample:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Attribute Subsets\n",
    "\n",
    "* Let $F$ be the size of random attribute subsets, where $F \\ge 2$\n",
    "\n",
    "> **Lab Task #2:** Define a Python function that selects $F$ random attributes from an attribute list\n",
    "\n",
    "(Note: You can do this with `np.random.choice()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['phd', 'level']\n",
      "[1, 0]\n"
     ]
    }
   ],
   "source": [
    "def compute_random_subset(values, num_values):\n",
    "    values_copy = values[:] # shallow copy\n",
    "    np.random.shuffle(values_copy) # in-place shuffling\n",
    "    return values_copy[:num_values]\n",
    "\n",
    "F = 2\n",
    "print(compute_random_subset(header, F))\n",
    "att_indexes = list(range(len(header)))\n",
    "print(compute_random_subset(att_indexes, F))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **THE BIG TAKEAWAY**\n",
    "> * $N$ - the number of weak ones\n",
    "> * $M$ - the number of best trees from the weak ones\n",
    "> * $F$ - size of the attribute subsets that we pass to select attribute"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a707b6ce8c685eb936424fcc3009d4b4b7a52543c4db09380a3fc49186ceb509"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
