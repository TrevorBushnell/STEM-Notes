{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture #5: Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UPDATE NOTES FROM LAST CLASS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization Using `numpy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "w = np.array([1.0, 2.5, -3.3])\n",
    "b = 4\n",
    "x = np.array([10, 20, 30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value for f is: -35.0\n"
     ]
    }
   ],
   "source": [
    "f = 0\n",
    "\n",
    "for i in range(len(x)):\n",
    "    f = f + w[i] * x[i]\n",
    "\n",
    "f += b\n",
    "\n",
    "print(\"The value for f is:\", f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Vectorization\n",
    "\n",
    "* Instead of writing an entire `for` loop, Python has features that allow you to perform operations across entire vectors!\n",
    "* Makes the code a lot shorter and more efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of f WITH VECTORIZATION is: -35.0\n"
     ]
    }
   ],
   "source": [
    "f = np.dot(w, x) + b\n",
    "print(\"The value of f WITH VECTORIZATION is:\", f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent with Multiple Features\n",
    "\n",
    "* There's a derivative with respect to every $w$ - so we need a vector of derivatives\n",
    "* Be sure to do this with vectorization so that the code is much cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the value of the vector w is: [0.47 1.28 3.36]\n"
     ]
    }
   ],
   "source": [
    "w = np.array([0.5, 1.3, 3.4])\n",
    "d = np.array([0.3, 0.2, 0.4])\n",
    "\n",
    "w = w - 0.1 * d\n",
    "print('the value of the vector w is:', w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient descent algorithm then becomes to repeat the following equations over and over again:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "w_j &= w_j - \\alpha \\frac{\\partial}{\\partial w_j} J(\\vec w, b)\\\\\n",
    "b &= b - \\alpha \\frac{\\partial}{\\partial b} J(\\vec w, b)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Then for multiple linear regression, repeat the following equations over and over again:\n",
    "\n",
    "$$\n",
    "w_j = w_j - \\frac{\\alpha}{m} \\sum_{i=1}^m (f_{\\vec w, b}(\\vec x^{(i)}) - y^{(i)})x_j^{(i)}\n",
    "$$\n",
    "\n",
    "* Where $j$ denotes the specific training example that we are on, we will loop through all the training examples in $x$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08849f8dffc3f27fb59cf06aefb79cb7a49147ad49564f36fdad5976b28849fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
