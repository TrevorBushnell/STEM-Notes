{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6: Gradient Descent In Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A: Feature Scaling Techniques\n",
    "\n",
    "* standardize the independent features in a fixed range\n",
    "* speed up the learning process\n",
    "* diabetes data:\n",
    "  * $x_1$: to serum cholesterol (range 50-300)\n",
    "  * $x_2$: total cholesterol/HDL (range 2-10)\n",
    "  * prediction: $w_1x_1 + w_2x_2 + b$\n",
    "  * one_training example: $x_1=157$, $x_2=4$, diabetes progression $=157$\n",
    "    * prediction: $w_1 = 0.5$, $w_2=10$, $b=60$\n",
    "      * prediction $= 0.5 \\cdot 157 + 10 \\cdot 4 + 60 = 158.5$ so pretty good prediction\n",
    "    * new prediction: $w_1 = 10$, $w_2 = 0.5$, $b=60$\n",
    "      * prediction value becomes way larger - 1632\n",
    "* Can help your prediction by *scaling all of your features*. Some options are:\n",
    "  * divide every data point by max\n",
    "  * divide every data point by the mean\n",
    "    * $x_{j, scaled}^{(i)} = \\frac{x_j^{i} - \\mu_j}{\\sigma_j}$\n",
    "  * calculate the z-score for every data point (need the mean and standard deviation of the data)\n",
    "* compute the learning curve of $J(\\vec w, b)$, make sure that it is converging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B: Checking Gradient Descent for Convergence\n",
    "\n",
    "* compute the learning curve of $J(\\vec w, b)$ to make sure that it's converging\n",
    "* use the **automatic convergence test**\n",
    "  * let $\\epsilon = 10^{-3}$\n",
    "  * if $J(\\vec w, b)$ decreases by $\\le \\epsilon$ in one iteration, then declare convergence\n",
    "* NOTE: The number of iterations varies by the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C: Choosing the Learning Rate\n",
    "\n",
    "* Be careful when choosing your learning rate that the following things do NOT happen:\n",
    "  * no bugs in your code\n",
    "    * if your $J(\\vec w, b)$ is bouncing around, there's likely a bug in your code because the nature of the algorithm provides a smooth curve\n",
    "  * $\\alpha$ is too big\n",
    "    * if $\\alpha$ is too big, then the values will bounce around and never reach the specific value, so adjust $\\alpha$ accordingly\n",
    "  * $\\alpha$ is too small\n",
    "    * in this case, the algorithm takes many iterations to converge and thus takes too long to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D: Feature Engineering\n",
    "\n",
    "* use intuition to design new features by transforming or combining original features\n",
    "* For example, if we have feature $x_1$ and $x_2$, then we could add a new feature $x_3 = \\frac{x_1}{x_2^2}$ and use this feature in our dataset\n",
    "  * Therefore we would go from $x_1x_1 + w_2x_2 + b \\implies x_1w_1 + x_2w_2 + w_3w_3 +  b$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08849f8dffc3f27fb59cf06aefb79cb7a49147ad49564f36fdad5976b28849fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
