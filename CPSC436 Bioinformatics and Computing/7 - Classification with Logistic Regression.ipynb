{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture #7: Classification with Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A: Motivation\n",
    "\n",
    "* If $y$ can only be one of two values, we have **binary classification**\n",
    "* linear regression is not good for classification problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B: The Model\n",
    "\n",
    "$$\n",
    "f_{\\vec w, b}(\\vec x) = g(\\vec w \\cdot \\vec x + b) = \\frac{1}{1+e^{-(\\vec w \\cdot \\vec x + b)}}\n",
    "$$\n",
    "\n",
    "* Note: $0 < g(z) < 1$ because this is a probability\n",
    "* The interpretation of the model: the probability that the feature is of the positive class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C: Decision Boundaries\n",
    "\n",
    "* You can have linear decision boundaries\n",
    "* You can also have non-linear decision boundaries\n",
    "  * ex: $f_{\\vec w, b}(\\vec x) = g(z) = g(w_1x_1^2 + w_2x^2_2 + b) \\ge 0.5$ (circular decision boundary)\n",
    "* more complex decision boundaries: $f_{\\vec w, b}(\\vec x) = g(z) = g(w_1x_1 + w_2x_2 + w_3x_1^2 + w_4x_1x_2 + w_5x^2_2+b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D: Cost Function for Logistic Regression\n",
    "\n",
    "* We cannot use squared error because then we would just stop at a local minimum rather than the absolute minimum (which is what we want)\n",
    "* As such, we use the following:\n",
    "\n",
    "$$\n",
    "J(\\vec w, b) = \\frac{1}{m} \\sum_{i=1}^m \\frac{1}{2} (f_{\\vec w, b}(\\vec x^{(i)})-y^{(i)})^2\n",
    "$$\n",
    "\n",
    "With loss function:\n",
    "\n",
    "$$\n",
    "L(f_{\\vec w, b}(\\vec x^{(i)}, y^{(i)})) = -\\log(f_{\\vec w, b}(\\vec x^{(i)}))\n",
    "$$\n",
    "\n",
    "if the positive class is 1\n",
    "\n",
    "OR\n",
    "$$\n",
    "L(f_{\\vec w, b}(\\vec x^{(i)}, y^{(i)})) = -\\log(f_{\\vec w, b}(1 - \\vec x^{(i)}))\n",
    "$$ \n",
    "\n",
    "if the positive class is 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08849f8dffc3f27fb59cf06aefb79cb7a49147ad49564f36fdad5976b28849fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
