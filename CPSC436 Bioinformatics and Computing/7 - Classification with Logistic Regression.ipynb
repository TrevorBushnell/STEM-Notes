{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture #7: Classification with Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A: Motivation\n",
    "\n",
    "* If $y$ can only be one of two values, we have **binary classification**\n",
    "* linear regression is not good for classification problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B: The Model\n",
    "\n",
    "$$\n",
    "f_{\\vec w, b}(\\vec x) = g(\\vec w \\cdot \\vec x + b) = \\frac{1}{1+e^{-(\\vec w \\cdot \\vec x + b)}}\n",
    "$$\n",
    "\n",
    "* Note: $0 < g(z) < 1$ because this is a probability\n",
    "* The interpretation of the model: the probability that the feature is of the positive class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C: Decision Boundaries\n",
    "\n",
    "* You can have linear decision boundaries\n",
    "* You can also have non-linear decision boundaries\n",
    "  * ex: $f_{\\vec w, b}(\\vec x) = g(z) = g(w_1x_1^2 + w_2x^2_2 + b) \\ge 0.5$ (circular decision boundary)\n",
    "* more complex decision boundaries: $f_{\\vec w, b}(\\vec x) = g(z) = g(w_1x_1 + w_2x_2 + w_3x_1^2 + w_4x_1x_2 + w_5x^2_2+b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D: Cost Function for Logistic Regression\n",
    "\n",
    "* We cannot use squared error because then we would just stop at a local minimum rather than the absolute minimum (which is what we want)\n",
    "* As such, we use the following:\n",
    "\n",
    "$$\n",
    "J(\\vec w, b) = \\frac{1}{m} \\sum_{i=1}^m \\frac{1}{2} (f_{\\vec w, b}(\\vec x^{(i)})-y^{(i)})^2\n",
    "$$\n",
    "\n",
    "With loss function:\n",
    "\n",
    "$$\n",
    "L(f_{\\vec w, b}(\\vec x^{(i)}, y^{(i)})) = -\\log(f_{\\vec w, b}(\\vec x^{(i)}))\n",
    "$$\n",
    "\n",
    "if the positive class is 1\n",
    "\n",
    "* As $f_{\\vec w, b} \\to 1$, then the loss $\\to 0$\n",
    "* As $f_{\\vec w, b} \\to 0$, then the loss $\\to \\infty$\n",
    "\n",
    "OR\n",
    "$$\n",
    "L(f_{\\vec w, b}(\\vec x^{(i)}, y^{(i)})) = -\\log(f_{\\vec w, b}(1 - \\vec x^{(i)}))\n",
    "$$ \n",
    "\n",
    "if the positive class is 0. \n",
    "\n",
    "* As $f_{\\vec w, b} \\to 0$, then the loss $\\to 0$\n",
    "* As $f_{\\vec w, b} \\to 1$, then the loss $\\to \\infty$\n",
    "\n",
    "A simplified loss function can be written as follows:\n",
    "\n",
    "$$\n",
    "L(f_{\\vec w, b}(\\vec x^{(i)}), y^{(i)}) = -y^{(i)} \\log(f_{\\vec w, b}(\\vec x^{(i)})) - (1 - y^{(i)}) \\log(1 - f_{\\vec w, b}(\\vec x^{(i)}))\n",
    "$$\n",
    "\n",
    "So the cost function becomes:\n",
    "\n",
    "$$\n",
    "J(\\vec w, b) = -\\frac{1}{m} \\sum_{i=1}^m [y^{(i)} \\log(f_{\\vec w, b}(\\vec x^{(i)})) + (1 - y^{(i)}) \\log(1 - f_{\\vec w, b}(\\vec x^{(i)}))]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E: Gradient Descent for Logistic Regression\n",
    "\n",
    "Repeat the following process:\n",
    "\n",
    "$$\n",
    "w_j = w_j - \\alpha \\frac{\\partial}{\\partial w_j} J(\\vec w, b)\n",
    "$$\n",
    "\n",
    "$$\n",
    "b = b - \\alpha \\frac{\\partial}{\\partial b}J(\\vec w, b)\n",
    "$$\n",
    "\n",
    "\n",
    "The above simplifies to the following:\n",
    "\n",
    "$$\n",
    "w_j = w_j - \\alpha \\frac{1}{m} \\sum_{i=1}^m [(f_{\\vec w, b}(\\vec x^{(i)})-y^{(i)})x_j^{(i)}]\n",
    "$$\n",
    "\n",
    "$$\n",
    "b = b - \\alpha \\frac{1}{m} \\sum_{i=1}^m [(f_{\\vec w, b}(\\vec x^{(i)})-y^{(i)})]\n",
    "$$\n",
    "\n",
    "* This looks the same as the linear regression, just be careful because the $f$ is going to be implemented differently!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08849f8dffc3f27fb59cf06aefb79cb7a49147ad49564f36fdad5976b28849fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
